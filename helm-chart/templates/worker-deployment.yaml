apiVersion: apps/v1
kind: Deployment
metadata:
  name: avatar-worker
  labels:
    {{- include "avatar.labels" . | nindent 4 }}
    app.kubernetes.io/component: worker

spec:
  selector:
    matchLabels:
      app.kubernetes.io/component: worker
  replicas: {{ .Values.resources.workerReplicas }}
  template:
    metadata:
      labels:
        {{- include "avatar.labels" . | nindent 8 }}
        app.kubernetes.io/component: worker
    spec:
      terminationGracePeriodSeconds: 3600 # We set it that high because we want to give the pod enough time to finish the task
      serviceAccountName: {{ .Values.gcp.ksaName | default "" | quote }}
      tolerations:
        # Allow this pod to be scheduled on tainted nodes with a key of "highMemory"
        # https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration
      - key: "highMemory"
        operator: "Exists"
      {{ if .Values.resources.workerPreferredNodeType }}
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: nodeType
                operator: In
                values:
                - {{ .Values.resources.workerPreferredNodeType | quote}}   
      {{ end }}
      containers:
        {{- include "avatar.db_proxy_container" $ }}
        - name: avatar-worker
          image: quay.io/octopize/api:{{ .Values.avatarVersion }}
          imagePullPolicy: {{ .Values.image.pullPolicy }}
          volumeMounts:
            - mountPath: /var/run/secrets
              name: secrets
              readOnly: true
          {{ if .Values.debug.storage.useLocal }}
            - mountPath: {{ .Values.api.sharedStoragePath }}
              name: avatar-pv-storage
          {{ end }}
          # This worker uses the default celery queue ('celery') and the small queue to handle small jobs if autoscaling with keda is enabled.
          # If autoscaling is not enabled, it listens to every queue.
          #
          # We are using 'args' and not 'command' as we want to use the entrypoint of the Dockerfile https://stackoverflow.com/a/49657024
          args: 
            # /quitquitquit is to shutdown the cloud-sql-proxy sidecar when the job terminates.
            # TODO: Replace the call to /quitquitquit when built-in support for Sidecar Containers is enabled
            # See https://github.com/kubernetes/enhancements/issues/753
            #
            # We set --without-mingle and --without-gossip because we don't want the workers on different
            # nodes communicating with each other.
            #
            # We use trap to forward the signals to the celery process with trap.
            - /bin/bash
            - -c
            - |
              celery -A api.lib.task worker --loglevel=info --without-mingle --without-gossip --pool=prefork --concurrency=4 {{- if .Values.worker.useKedaAutoscaler }} -Q celery,small {{- end }} &
              CELERY_PID=$!
              trap 'kill -TERM $CELERY_PID' SIGTERM
              trap 'kill -KILL $CELERY_PID' SIGKILL
              wait $CELERY_PID
              {{- if $.Values.gcp.dbInstanceConnectionName }}
              sleep 10 # Give celery a few seconds to shutdown and cleanup
              curl --silent --show-error --retry 3 --retry-connrefused -X POST localhost:9091/quitquitquit
              {{- end }}
          resources:
            requests:
              memory: {{ .Values.resources.workerMemoryRequest }}
              cpu: {{ .Values.resources.workerCpuRequest }}
            limits:
              memory: {{ .Values.resources.workerMemoryRequest }}
              cpu: {{ .Values.resources.workerCpuRequest }}
          env:
            {{- include "avatar.app_env" . }}
            # We do not want for celery to wait for long running tasks to finish, which is the default
            # behavior for SIGTERM. We want to use SIGQUIT instead, which initiates a Cold Shutdown,
            # and does not wait for tasks to finish.
            - name: REMAP_SIGTERM
              value: "SIGQUIT"
            {{ if .Values.debug.storage.useLocal }}
            - name: USER_ID
              value: "1000"
            - name: GROUP_ID
              value: "1000"
            - name: USE_CONSOLE_LOGGING
              value: "True"
            {{ end }}

      imagePullSecrets:
        - name: docker-local-pull-secret
      volumes:
        - name: secrets
          secret:
            secretName: api
            # We specifically list only the secrets that are needed in the image
            items:
            - key: db_name
              path: db_name
            - key: db_user
              path: db_user
            - key: db_password
              path: db_password
            - key: file_encryption_key
              path: file_encryption_key
    {{ if .Values.debug.storage.useLocal }}
        - name: avatar-pv-storage
          persistentVolumeClaim:
            claimName: avatar-pv-claim
      securityContext:
        runAsUser: 0
    {{ end }}